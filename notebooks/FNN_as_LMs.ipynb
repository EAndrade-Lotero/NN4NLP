{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "This notebook will develop a feedforward neural network, optimizing it to achieve superior performance in language modeling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "\n",
    "When starting the notebook or restarting the kernel, all dependencies can be loaded by running the following cells. This is also the place to install any missing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# import sys\n",
    "# from pathlib import Path\n",
    "\n",
    "# path_notebook = Path('/content/drive', 'PATH_TO_NOTEBOOK')\n",
    "# sys.path.append(path_notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# !python -m spacy download en_core_web_sm \n",
    "# !python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c6cdc5-0bbf-4533-9d53-2e29b469511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook-specific dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'NN4NLP'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mNN4NLP\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m print_\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mNN4NLP\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PATHS\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mNN4NLP\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils_vocab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BasicTokenizer, CustomDataset\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'NN4NLP'"
     ]
    }
   ],
   "source": [
    "from NN4NLP.utils.utils import print_\n",
    "from NN4NLP.config.config import PATHS\n",
    "from NN4NLP.utils.utils_vocab import BasicTokenizer, CustomDataset\n",
    "from NN4NLP.utils.utils_visualization import NN4NLPPlots\n",
    "from NN4NLP.utils.utils_training import NN4NLPTrainer\n",
    "from NN4NLP.models.nn_models import FFNLanguageModeler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03d6443-ed64-4f33-843d-762fa3a2605d",
   "metadata": {},
   "source": [
    "# Sections\n",
    "\n",
    "1. [Overview of the network](#red)\n",
    "2. [Feedforward network](#ffn)\n",
    "3. [Training](#training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4927d8-5a3b-4181-9c82-5aca43353744",
   "metadata": {},
   "source": [
    "# Overview of the network <a class=\"anchor\" id=\"red\"></a>\n",
    "\n",
    "Feedforward Neural Networks (FNNs), also known as Multilayer Perceptrons, form the fundamental basis for understanding neural networks in natural language processing (NLP). In NLP tasks, these networks process textual data by converting it into numerical vectors called embeddings. These embeddings are then fed into the network to predict various aspects of language, such as the next word in a sentence or the sentiment expressed in a text.\n",
    "\n",
    "We will start by creating an FNN for some very simple sequential data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79a168-6490-4ebf-9ff0-5e6ce3ea5be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_data = [str(x) for x in range(10)]\n",
    "print_(sequential_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we initialize the tokenizer. Note that this time we use a very simple procedure, using the `split` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "special_symbols = ['<UNK>', '<pad>', '<s>', '</s>']\n",
    "simple_tokenizer = lambda text: text.split(' ')\n",
    "tokenizer = BasicTokenizer(simple_tokenizer, special_symbols)\n",
    "tokenizer.initialize_from_iterable(sequential_data)\n",
    "print(f'Cantidad de tokens en el tokenizer: {tokenizer.get_vocab_size()}')\n",
    "print_(tokenizer.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c3cfe2-0c57-470e-968a-eb6f4de9c580",
   "metadata": {},
   "source": [
    "We organize the words within a variable-sized context using the following approach: each word is represented by `i`. To establish the context, simply subtract `j` within the range defined by the `CONTEXT_SIZE` value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810d4eed-1995-4f89-8059-7b1579722b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "\n",
    "ngrams = [\n",
    "    (\n",
    "        [sequential_data[i - j - 1] for j in range(CONTEXT_SIZE - 1, -1, -1)],\n",
    "        sequential_data[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(sequential_data))\n",
    "]\n",
    "print_(ngrams[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network `FFNLanguageModeler`, located in the `NN4NLP.models.nn_models` module, is a language model based on a feedforward neural network (FNN), designed to predict the next word in a sequence from a fixed context. Let's analyze its structure and the data flow through its layers:\n",
    "\n",
    "1. Input: A sequence of `CONTEXT_SIZE` words represented by their indices in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded57d2c-ce91-4a0c-8901-200b0ba87e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context, target = ngrams[1]\n",
    "print(\"context:\", context)\n",
    "print(\"context index:\", tokenizer.encode(context).ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Conversion to embeddings: Each word is converted into a dense vector of size `embedding_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcef394a-0412-4224-8bce-7aa44c883ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 2\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "embeddings = nn.Embedding(vocab_size, embedding_dim) # <= se usa la capa Embedding de Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad3b78-2464-4ff5-a531-9045e9604ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in tokenizer.encode(context).ids: \n",
    "    embedding = embeddings(torch.tensor(n))\n",
    "    print(\"word\", tokenizer.itos[n])\n",
    "    print(\"index\", n)\n",
    "    print( \"embedding\", embedding)\n",
    "    print(\"embedding shape\", embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5afea5-8ac4-4b2e-b532-9b693f3959ab",
   "metadata": {},
   "source": [
    "3. Concatenation: The embeddings are joined into a single input vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07371f3-e59d-493e-aa23-881504fc279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_embeddings = embeddings(torch.tensor(tokenizer.encode(context).ids))\n",
    "my_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f6ff53-db10-4899-b5b3-dc12674f5152",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_embeddings = my_embeddings.reshape(1,-1)\n",
    "my_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42bebc5-0450-429f-a583-ba7c16d7f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 6\n",
    "linear1 = nn.Linear(embedding_dim*CONTEXT_SIZE, HIDDEN_SIZE) # <= se usa la capa Linear de Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdc2481-9f4b-4470-9e80-802c8ebbe541",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_output = linear1(my_embeddings)\n",
    "hidden_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Non-linear transformation: The vector is passed through a hidden layer with ReLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_output = F.relu(hidden_output)\n",
    "hidden_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Word prediction: The final output is a vector of logits of size `vocab_size`, representing the score for each word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear2 = nn.Linear(HIDDEN_SIZE, vocab_size) # <= se usa la capa Linear de Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = linear2(hidden_output)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything into a pipeline\n",
    "\n",
    "We just looked in detail at the steps followed by the network. But all of them must come together for the creation, training, and evaluation of the model.\n",
    "\n",
    "The first thing we need to do in the pipeline, after creating the tokenizer, is to create the dataloader. Note that this dataloader requires a number of examples that can be evenly distributed across batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c9d29b-2e7b-4cb9-9c83-328cfe8b4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = NN4NLPTrainer.get_device()\n",
    "print(f'Device encontrado: {device}')\n",
    "\n",
    "CONTEXT_SIZE = 2\n",
    "BATCH_SIZE = 4\n",
    "EMBEDDING_DIM = 2\n",
    "HIDDEN_SIZE = 6\n",
    "\n",
    "Padding = len(sequential_data) % BATCH_SIZE\n",
    "tokens_pad = sequential_data + sequential_data[:Padding] # <= Se uniforma el último batch\n",
    "\n",
    "ngrams = [\n",
    "    (\n",
    "        [tokens_pad[i - j - 1] for j in range(CONTEXT_SIZE - 1, -1, -1)],\n",
    "        tokens_pad[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(tokens_pad))\n",
    "]\n",
    "\n",
    "dataset = CustomDataset(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the collate function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b76776-5962-4d59-ab84-02f3113e8dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    context_list, target_list = list(), list()\n",
    "    for context, target in batch:\n",
    "        target_id = tokenizer.encode([target]).ids\n",
    "        context_ids = tokenizer.encode(context).ids\n",
    "        context_ids = torch.tensor(context_ids, dtype=torch.int64)\n",
    "        target_list.append(target_id)\n",
    "        context_list.append(context_ids)\n",
    "\n",
    "    target_list = torch.tensor(target_list, dtype=torch.int64)\n",
    "    context_list = torch.cat(context_list)\n",
    "    return context_list.to(device), target_list.to(device).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc056f8-1965-46cf-b1b5-3891b348ae6c",
   "metadata": {},
   "source": [
    "We create the `DataLoader`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd79764-c476-4815-b346-d2b62cacb629",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "     dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "print(f'Tamaño del dataloader: {len(dataloader)}')\n",
    "\n",
    "print('')\n",
    "print('--- Un batch de ejemplo ---')\n",
    "for context, target in dataloader:\n",
    "     print(f'Tamaño del contexto: {context.shape}')\n",
    "     print(f'Tamaño del target: {target.shape}')\n",
    "     print(f\"context: {context}\")\n",
    "     print(f\"target: {target}\")\n",
    "     print(f\"context decodificado: {tokenizer.decode(context)}\")\n",
    "     print(f\"target decodificado: {tokenizer.decode(target)}\")\n",
    "     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b38ee0e-ece3-414c-b35e-ab1a09504f00",
   "metadata": {},
   "source": [
    "# Feedforward network <a class=\"anchor\" id=\"ffn\"></a>\n",
    "\n",
    "We have already implemented the neural network in PyTorch in the `FFNLanguageModeler` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7a4221-601a-4a96-b462-073e11b44ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFNLanguageModeler(\n",
    "    vocab_size=vocab_size, \n",
    "    embedding_dim=EMBEDDING_DIM, \n",
    "    hidden_size=HIDDEN_SIZE, \n",
    "    context_size=CONTEXT_SIZE\n",
    ").to(device)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f55c82-95df-4862-b39e-c824d07ce056",
   "metadata": {},
   "source": [
    "Note that the network receives an entire batch obtained from the dataloader and returns the prediction of the next word:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb02f395-a671-4512-bf4e-0d062d62e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "context, target = next(iter(dataloader))\n",
    "print(f\"context decodificado: {tokenizer.decode(context)}\")\n",
    "print(f\"target decodificado: {tokenizer.decode(target)}\")\n",
    "out = model(context)\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b061927-2268-4820-981b-ce855fa114c6",
   "metadata": {},
   "source": [
    "In the output, the first dimension corresponds to the batch size, while the second dimension represents the probability of the next word.\n",
    "\n",
    "To predict the next word, we need to find the index with the highest probability. This is done for each of the datapoints in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f993db93-fb10-4e15-b227-91368f44e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_index = torch.argmax(out,1)\n",
    "predicted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8daac7-247d-4da8-8a73-60324ef72893",
   "metadata": {},
   "source": [
    "We find the corresponding token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc89c2dd-9316-4dfe-b4f5-a4e9ea26b08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([i.item() for i in  predicted_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970bf4a3-8719-4e93-80ef-11e434f68817",
   "metadata": {},
   "source": [
    "The following is a function that generates tokens from a given context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32743b22-af82-4290-84d6-7853a9e2a535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar(model, context=None, number_of_words=10):\n",
    "    model.eval()\n",
    "    if context is None:\n",
    "        context = [str(x) for x in range(CONTEXT_SIZE)]\n",
    "    my_gen = ' '.join(context)\n",
    "    for i in range(number_of_words):\n",
    "        with torch.no_grad():\n",
    "            tokens_ids = tokenizer.encode(context[-CONTEXT_SIZE:]).ids\n",
    "            prediction = model(torch.tensor(tokens_ids).to(device))\n",
    "            word_indx = torch.argmax(prediction)\n",
    "            word = tokenizer.decode([word_indx.detach().item()])[0]\n",
    "            context.append(word)\n",
    "            my_gen += \" \" + word\n",
    "\n",
    "    return my_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f9e70d-e2a4-4a13-8661-4abd4a5b88f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generar(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e19330-6df7-4fb0-bd69-1ab0ea110a98",
   "metadata": {},
   "source": [
    "# Training <a class=\"anchor\" id=\"training\"></a>\n",
    "\n",
    "To train the network, follow these steps:\n",
    "\n",
    "1. **Set up the loss function and optimizer**:  \n",
    "   - Use `nn.CrossEntropyLoss` for word classification.  \n",
    "   - Employ an optimizer such as `Adam` or `SGD`.  \n",
    "\n",
    "2. **Train the model**:  \n",
    "   - For each data batch:  \n",
    "     - Convert the context into embeddings.  \n",
    "     - Perform forward propagation.  \n",
    "     - Compute the loss and perform backpropagation.  \n",
    "     - Update the model weights.  \n",
    "\n",
    "5. **Evaluate the model**:  \n",
    "   - Measure accuracy on validation data.  \n",
    "   - Adjust hyperparameters if necessary.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e67467-af80-42a2-a0cd-67a30b356d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer for training the model, using stochastic gradient descent (SGD)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Set up a learning rate scheduler using StepLR to adjust the learning rate during training\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1.0, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a108808-4bac-4eaf-8ace-cb5285516b92",
   "metadata": {},
   "source": [
    "We train the model. This model is small and the amount of data is also small. The training should take less than a minute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, epoch_losses = NN4NLPTrainer.train_lm_model(\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=1500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_losses)\n",
    "plt.xlabel(\"epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9effde4-0544-49fd-86a0-034d9c6aabcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name =  PATHS['lms'] / Path('lm_fnn.pt')\n",
    "torch.save(model.state_dict(), file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the following cell if we encountered issues during training, to load a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Load model\n",
    "# --------------------------------------\n",
    "model = model = FFNLanguageModeler(\n",
    "    vocab_size=vocab_size, \n",
    "    embedding_dim=EMBEDDING_DIM, \n",
    "    hidden_size=HIDDEN_SIZE, \n",
    "    context_size=CONTEXT_SIZE\n",
    ")\n",
    "file_name =  PATHS['lms'] / Path('lm_fnn_pretrained.pt')\n",
    "state_dict = torch.load(file_name)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the generate function on the context ['0', '1']:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generar(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "\n",
    "Your mission is to train a language model to reproduce Master Yoda's phrases, as discussed in the *N-grams as LMs* notebook. To do this, follow these steps:\n",
    "\n",
    "1. Create a spaCy tokenizer using Master Yoda's phrases as the vocabulary.\n",
    "2. Create a dataloader with Master Yoda's phrases. Use the following hyperparameters:\n",
    "    - CONTEXT_SIZE = 5\n",
    "    - BATCH_SIZE = 32\n",
    "3. Create an ``FFNLanguageModeler`` with the following hyperparameters:\n",
    "    - EMBEDDING_DIM = 64\n",
    "    - HIDDEN_SIZE = 128\n",
    "4. Train the model for 100 epochs using the same hyperparameters as the ``sequential_data`` model.\n",
    "5. Generate sentences from the following contexts:\n",
    "    - Abandonarte la Fuerza no puede\n",
    "    - un paso delante de nosotros\n",
    "6. Compute the model's perplexity on its training data. Compare it to the perplexity of the trigram model obtained in the *N-grams as LMs* notebook.\n",
    "\n",
    "**Expected time**: 6 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "prev_pub_hash": "2f3a760070e26b6682d94eed9766f4247e3c53a584ce883caaf62ec4b5e8b61d"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
